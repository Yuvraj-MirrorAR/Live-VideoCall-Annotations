import sys

sys.path.append("packages/")
import argparse
import asyncio
from asyncio.windows_events import NULL
import json
import logging
import os
import ssl
import uuid
import mediapipe as mp
import time
# import tflitemodel as mytfLite
import pose_estimation_class as pm
import twilio.jwt.access_token
import twilio.jwt.access_token.grants
import twilio.rest
from dotenv import load_dotenv
# import tensorflow as tf

import cv2
from aiohttp import web
from av import VideoFrame

from aiortc import MediaStreamTrack, RTCPeerConnection, RTCSessionDescription
from aiortc.contrib.media import MediaBlackhole, MediaPlayer, MediaRecorder, MediaRelay

ROOT = os.path.dirname(__file__)

logger = logging.getLogger("pc")
pcs = set()
relay = MediaRelay()

face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

mlModelName = "pose"
# Twilio Functions and Variables

# from torchvision.models import detection
# from imutils.video import VideoStream
# from imutils.video import FPS


# Load environment variables from a .env file
load_dotenv()

# Create a Twilio client
account_sid = "AC5666adc265c82bdac1292090abb4c37c"
api_key = "SK524d1beb703e898206f21959e4823816"
api_secret = "iWaGvl0B6alw2Bqe47VjDgkiMHfFB7l2"
twilio_client = twilio.rest.Client(api_key, api_secret, account_sid)



detector = pm.PoseDetector()
def find_or_create_room(room_name):
    try:
        # try to fetch an in-progress room with this name
        twilio_client.video.rooms(room_name).fetch()
      
    except twilio.base.exceptions.TwilioRestException:
        # the room did not exist, so create it
        twilio_client.video.rooms.create(unique_name=room_name, type="go")


def get_access_token(room_name, user_name):
    # create the access token
    access_token = twilio.jwt.access_token.AccessToken(
        account_sid, api_key, api_secret, identity=user_name
    )
    # create the video grant
    video_grant = twilio.jwt.access_token.grants.VideoGrant(room=room_name)
    # Add the video grant to the access token
    access_token.add_grant(video_grant)
    return access_token





async def join_room(request):
   
    params = await request.json()
    
    # extract the room_name from the JSON body of the POST request
    room_name = params['room_name']
    user_name = params['user_name']
    mlModelName = params['model_name']
    print(mlModelName)
    # find an existing room with this room_name, or create one
    find_or_create_room(room_name)
    # retrieve an access token for this room
    access_token = get_access_token(room_name, user_name)
    print(access_token)
    
    return web.Response(
        content_type="application/json",
        text=json.dumps(
            {"token": access_token.to_jwt().decode()}
        ),
    )
    # return the decoded access token in the response
    # return {"token": access_token.to_jwt().decode()}





# # Load TFLite model and allocate tensors.
# interpreter = tf.lite.Interpreter(model_path="coco_ssd_mobilenet/detect.tflite")
# interpreter.allocate_tensors()

# # Get input and output tensors.
# input_details = interpreter.get_input_details()
# output_details = interpreter.get_output_details()

# category_index = mytfLite.create_category_index()
# input_shape = input_details[0]['shape']


class VideoTransformTrack(MediaStreamTrack):
    """
    A video stream track that transforms frames from an another track.
    """
    
    kind = "video"
    
    def __init__(self, track, transform):
        super().__init__()  # don't forget this!
        self.track = track
        self.transform = transform
        self.pTime = 0
        self.skip_frame_cnt = 2
        self.count = 0
        self.lastFrame = NULL
        self.countAll = 0
        self.frameCount = 0
        self.lastTimerCount = 0
        self.annotatedLastImage = False
      

    async def recv(self):
        now = time.time()
      

        frame = await self.track.recv()
        img = frame.to_ndarray(format="bgr24")
        mlModelName = self.transform
        if self.count == 0:
            self.lastFrame = frame
        self.count += 1
        now = time.time()
        if self.lastTimerCount == 0 :
            self.lastTimerCount = now
        else: 
            self.frameCount = self.frameCount + 1
            self.countAll = self.countAll + (now - self.lastTimerCount)
            print("Time between frames is: {} sec".format(self.countAll/self.frameCount))
            self.lastTimerCount = now
          
        if self.count % self.skip_frame_cnt != 0: return self.lastFrame# skip this frame 
        # print(mlModelName)

        if mlModelName == "cartoon":
            mytfLite.make_and_show_inference(img, interpreter, input_details, output_details, category_index)
            new_frame = VideoFrame.from_ndarray(img, format="bgr24")
            new_frame.pts = frame.pts
            new_frame.time_base = frame.time_base
            # new_frame = self.MakeCartoon(frame, img)
            # self.lastFrame = new_frame
            return new_frame
        elif mlModelName == "text":
            new_frame = self.AddTextOnImage(frame, img)
            self.lastFrame = new_frame
            then = time.time()
            # if self.lastTimerCount == 0 :
            #     self.lastTimerCount = then
            # else: 
            #     self.frameCount = self.frameCount + 1
            #     self.countAll = self.countAll + (then - self.lastTimerCount)
            #     print("Time between frames is: {} sec".format(self.countAll/self.frameCount))
            #     self.lastTimerCount = then
            # print("Detected pose in: {} sec".format(then - now))
           
            return new_frame
        elif mlModelName == "face":
            new_frame = self.DetectFace(frame, img)
            then = time.time()
            # if self.lastTimerCount == 0 :
            #     self.lastTimerCount = then
            # else: 
            #     self.frameCount = self.frameCount + 1
            #     self.countAll = self.countAll + (then - self.lastTimerCount)
            #     print("Time between frames is: {} sec".format(self.countAll/self.frameCount))
            #     self.lastTimerCount = then
            # print("Detected pose in: {} sec".format(then - now))
           
            self.lastFrame = new_frame
            return new_frame
        elif mlModelName == "edge":
            new_frame = self.DetectEdge(frame, img)
            self.lastFrame = new_frame
            return new_frame
        elif mlModelName == "pose":
            new_frame = self.PoseDetection(frame, img) 
            then = time.time()
            # if self.lastTimerCount == 0 :
            #     self.lastTimerCount = then
            # else: 
            #     self.frameCount = self.frameCount + 1
            #     self.countAll = self.countAll + (then - self.lastTimerCount)
            #     print("Time between frames is: {} sec".format(self.countAll/self.frameCount))
            #     self.lastTimerCount = then
            # print("Detected pose in: {} sec".format(then - now))
            self.lastFrame = new_frame
            self.annotatedLastImage = True
            return new_frame   
        else:
            self.lastFrame = new_frame
            return frame

    def AddTextOnImage(self, frame, img):
        text = "Patient"
        img = cv2.putText(img, text, (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 1, cv2.LINE_AA) 

            # rebuild a VideoFrame, preserving timing information
        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame

    def DetectEdge(self, frame, img):
        img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)
        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame

    def PoseDetection(self, frame, img):
        img, p_landmarks, p_connections = detector.findPose(img, False)
    
       
        # draw points
        mp.solutions.drawing_utils.draw_landmarks(img, p_landmarks, p_connections)
        # lmList = detector.getPosition(img)
       

        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame

    def DetectFace(self, frame, img):
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        # Draw rectangle around the faces
        for (x, y, w, h) in faces:
            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
        
      
        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame

    def MakeCartoon(self, frame, img):
        # prepare color
        img_color = cv2.pyrDown(cv2.pyrDown(img))
        for _ in range(6):
            img_color = cv2.bilateralFilter(img_color, 9, 9, 7)
        img_color = cv2.pyrUp(cv2.pyrUp(img_color))

        # prepare edges
        img_edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        img_edges = cv2.adaptiveThreshold(
        cv2.medianBlur(img_edges, 7),
            255,
            cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,
            9,
            2,
        )
        img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)

        # combine color and edges
        img = cv2.bitwise_and(img_color, img_edges)
        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame




async def index(request):
    content = open(os.path.join(ROOT, "templates/index.html"), "r", encoding="utf8").read()
    return web.Response(content_type="text/html", text=content)


async def javascript(request):
    content = open(os.path.join(ROOT, "static/client.js"), "r", encoding="utf8").read()
    return web.Response(content_type="application/javascript", text=content)


async def offer(request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])

    pc = RTCPeerConnection()
    pc_id = "PeerConnection(%s)" % uuid.uuid4()
    pcs.add(pc)

    def log_info(msg, *args):
        logger.info(pc_id + " " + msg, *args)

    log_info("Created for %s", request.remote)

    # prepare local media
    # player = MediaPlayer(os.path.join(ROOT, "demo-instruct.wav"))
    # if args.record_to:
    #     recorder = MediaRecorder(args.record_to)
    # else:
    #     recorder = MediaBlackhole()

    @pc.on("datachannel")
    def on_datachannel(channel):
        @channel.on("message")
        def on_message(message):
            if isinstance(message, str) and message.startswith("ping"):
                channel.send("pong" + message[4:])

    @pc.on("connectionstatechange")
    async def on_connectionstatechange():
        log_info("Connection state is %s", pc.connectionState)
        if pc.connectionState == "failed":
            await pc.close()
            pcs.discard(pc)

    @pc.on("track")
    def on_track(track):
        log_info("Track %s received", track.kind)

      
        if track.kind == "video":
            pc.addTrack(
                VideoTransformTrack(
                    relay.subscribe(track),transform=params["video_transform"]
                )
            )
            print("AYA")
            # if args.record_to:
            #     recorder.addTrack(relay.subscribe(track))

        @track.on("ended")
        async def on_ended():
            log_info("Track %s ended", track.kind)
            # await recorder.stop()

  

    # handle offer
    await pc.setRemoteDescription(offer)
    # await recorder.start()

    # send answer
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)



    return web.Response(
        content_type="application/json",
        text=json.dumps(
            {"sdp": pc.localDescription.sdp, "type": pc.localDescription.type}
        ),
    )


async def on_shutdown(app):
    # close peer connections
    coros = [pc.close() for pc in pcs]
    await asyncio.gather(*coros)
    pcs.clear()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="WebRTC audio / video / data-channels demo"
    )
    parser.add_argument("--cert-file", help="SSL certificate file (for HTTPS)")
    parser.add_argument("--key-file", help="SSL key file (for HTTPS)")
    parser.add_argument(
        "--host", default="0.0.0.0", help="Host for HTTP server (default: 0.0.0.0)"
    )
    parser.add_argument(
        "--port", type=int, default=8080, help="Port for HTTP server (default: 8080)"
    )
    # parser.add_argument("--record-to", help="Write received media to a file."),
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    if args.cert_file:
        ssl_context = ssl.SSLContext()
        ssl_context.load_cert_chain(args.cert_file, args.key_file)
    else:
        ssl_context = None

    app = web.Application()
    app.on_shutdown.append(on_shutdown)
    app.router.add_get('/', index)
    app.router.add_get('/static/client.js', javascript)
    app.router.add_post("/offer", offer)
    app.router.add_post("/join_room", join_room)
    web.run_app(
        app, access_log=None, host=args.host, port=args.port, ssl_context=ssl_context
    )


